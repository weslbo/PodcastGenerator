[Brian]: So Emma, I've been hearing a lot about working with delta tables in Spark. Can you explain how we can use Spark SQL to work with data in delta tables?

[Emma]: Absolutely, Brian. Spark SQL is the most common way to work with data in delta tables in Spark. You can embed SQL statements in other languages like PySpark or Scala using the `spark.sql` library. For example, you can insert a row into the `products` table using the following code.

[Brian]: That's interesting, Emma. Is there any other way to work with delta files instead of catalog tables?

[Emma]: Yes, Brian. If you prefer working with delta files directly, you can use the Delta Lake API. You can create a `DeltaTable` instance from a folder location containing delta format files and then use the API to modify the data in the table.

[Brian]: That sounds convenient, Emma. Now, I've heard about time travel in delta tables. Can you explain how it works?

[Emma]: Of course, Brian. Time travel allows you to view the history of changes made to a delta table and retrieve older versions of the data. The modifications made to delta tables are logged in the transaction log. You can use SQL commands to see the history of a table and the transactions applied to it.

[Brian]: That's really useful, Emma. How can we retrieve data from a specific version of the data in a delta table?

[Emma]: To retrieve data from a specific version, you can read the delta file location into a dataframe and specify the version as an option. Alternatively, you can also specify a timestamp using the option.

[Brian]: Got it, Emma. Thanks for explaining how we can work with delta tables in Spark using Spark SQL and the Delta Lake API, as well as how time travel can be used for table versioning.